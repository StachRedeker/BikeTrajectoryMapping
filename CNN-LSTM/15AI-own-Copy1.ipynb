{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5bce96-1628-4b59-8d36-de95a2e11305",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 11:41:08.827657: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 11:41:10.176278: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-14 11:41:10.176414: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-14 11:41:10.176430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                    date/time     gyro_x    gyro_y    gyro_z   accel_x  \\\n",
       " 0  2024-06-05 14:09:12.810708  47.229008  1.580153  4.488550  0.464475   \n",
       " 1  2024-06-05 14:09:12.818031  47.030534  0.419847  5.343511 -0.964863   \n",
       " 2  2024-06-05 14:09:12.824984  47.328244 -4.145038  6.557252 -0.799663   \n",
       " 3  2024-06-05 14:09:12.832316  48.068702 -3.725191  6.763359  1.841141   \n",
       " 4  2024-06-05 14:09:12.839155  47.679389 -1.847328  6.595420  3.287239   \n",
       " \n",
       "     accel_y    accel_z          x          y                   timestamp  \\\n",
       " 0  0.493206   7.393295  24.801591  53.530177  2024-06-05 14:09:12.810708   \n",
       " 1  0.105345   7.541735  24.776822  53.538791  2024-06-05 14:09:12.818031   \n",
       " 2  0.849942   8.384494  24.752052  53.547405  2024-06-05 14:09:12.824984   \n",
       " 3  0.289698  10.572795  24.727283  53.556018  2024-06-05 14:09:12.832316   \n",
       " 4  0.694318  11.298238  24.702514  53.564632  2024-06-05 14:09:12.839155   \n",
       " \n",
       "       x_cam     y_cam     z_cam                   datetime  \n",
       " 0  0.469269 -0.780899  0.338817 2024-06-05 14:09:12.810708  \n",
       " 1  0.469303 -0.780876  0.338859 2024-06-05 14:09:12.818031  \n",
       " 2  0.469336 -0.780852  0.338900 2024-06-05 14:09:12.824984  \n",
       " 3  0.469370 -0.780828  0.338941 2024-06-05 14:09:12.832316  \n",
       " 4  0.469404 -0.780804  0.338982 2024-06-05 14:09:12.839155  ,\n",
       "                     date/time      gyro_x     gyro_y     gyro_z   accel_x  \\\n",
       " 0  2024-06-12 10:39:01.386113    0.000000   0.297710   9.480916  0.000000   \n",
       " 1  2024-06-12 10:39:01.393501 -164.664122  47.977099  26.106870 -0.802058   \n",
       " 2  2024-06-12 10:39:01.400725 -197.076336  39.099237  24.137405 -0.897826   \n",
       " 3  2024-06-12 10:39:01.407921   47.549618  -0.908397  -0.427481 -0.718260   \n",
       " 4  2024-06-12 10:39:01.415058   48.938931  -1.068702  -1.389313 -0.866701   \n",
       " \n",
       "     accel_y   accel_z          x         y                   timestamp  \\\n",
       " 0  0.000000  0.000000  19.844030  7.616852  2024-06-12 10:39:01.386113   \n",
       " 1  0.426168  8.702923  19.844241  7.617198  2024-06-12 10:39:01.393501   \n",
       " 2  0.517148  8.765172  19.844453  7.617543  2024-06-12 10:39:01.400725   \n",
       " 3  0.402226  8.652645  19.844664  7.617888  2024-06-12 10:39:01.407921   \n",
       " 4  0.440533  8.762778  19.844875  7.618234  2024-06-12 10:39:01.415058   \n",
       " \n",
       "       x_cam     y_cam     z_cam                   datetime  \n",
       " 0 -0.983697 -0.179833 -0.000661 2024-06-12 10:39:01.386113  \n",
       " 1 -0.983713 -0.179845 -0.000703 2024-06-12 10:39:01.393501  \n",
       " 2 -0.983729 -0.179856 -0.000746 2024-06-12 10:39:01.400725  \n",
       " 3 -0.983745 -0.179868 -0.000788 2024-06-12 10:39:01.407921  \n",
       " 4 -0.983761 -0.179879 -0.000830 2024-06-12 10:39:01.415058  ,\n",
       "                         date/time     gyro_x    gyro_y    gyro_z   accel_x  \\\n",
       " 98932  2024-06-12 10:51:18.307869  38.709924  7.305344  9.229008 -1.731008   \n",
       " 98933  2024-06-12 10:51:18.315934  36.793893  5.114504  9.648855 -4.005499   \n",
       " 98934  2024-06-12 10:51:18.322856  35.381679  4.900763  6.129771 -3.873818   \n",
       " 98935  2024-06-12 10:51:18.329749  33.267176  4.603053  6.885496 -2.918532   \n",
       " 98936  2024-06-12 10:51:18.336650  36.557252  1.763359  6.259542 -3.043030   \n",
       " \n",
       "         accel_y   accel_z          x          y                   timestamp  \\\n",
       " 98932  0.826000  8.901642  15.442615  53.166534  2024-06-12 10:51:18.307869   \n",
       " 98933  1.920150  7.096414  15.420734  53.152238  2024-06-12 10:51:18.315934   \n",
       " 98934  0.703895  7.733271  15.398852  53.137941  2024-06-12 10:51:18.322856   \n",
       " 98935  0.028730  8.176199  15.376971  53.123645  2024-06-12 10:51:18.329749   \n",
       " 98936  0.191536  8.269573  15.355089  53.109348  2024-06-12 10:51:18.336650   \n",
       " \n",
       "              x_cam        y_cam       z_cam                   datetime  \n",
       " 98932 -1688.013761  1337.318013  311.222472 2024-06-12 10:51:18.307869  \n",
       " 98933 -1687.817928  1337.358629  311.222580 2024-06-12 10:51:18.315934  \n",
       " 98934 -1687.622096  1337.399244  311.222688 2024-06-12 10:51:18.322856  \n",
       " 98935 -1687.485264  1337.608474  311.223072 2024-06-12 10:51:18.329749  \n",
       " 98936 -1687.348433  1337.817704  311.223455 2024-06-12 10:51:18.336650  )"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the file paths\n",
    "train_files = ['own/data-int/1.csv', 'own/data-int/2.csv', 'own/data-int/5.csv']\n",
    "validation_test_file = 'own/data-int/4.csv'\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(train_files, validation_test_file):\n",
    "    # Load the training data\n",
    "    train_dfs = [pd.read_csv(file) for file in train_files]\n",
    "    \n",
    "    # Load the validation and test data\n",
    "    validation_test_df = pd.read_csv(validation_test_file)\n",
    "    \n",
    "    # Convert 'date/time' columns to datetime\n",
    "    for df in train_dfs:\n",
    "        df['datetime'] = pd.to_datetime(df['date/time'], errors='coerce')\n",
    "    validation_test_df['datetime'] = pd.to_datetime(validation_test_df['date/time'], errors='coerce')\n",
    "    \n",
    "    # Process each training dataframe\n",
    "    processed_train_dfs = []\n",
    "    for df, file in zip(train_dfs, train_files):\n",
    "        if '1.csv' in file:\n",
    "            # Remove the first 1 minute and the last 3 minutes from the first training data (1.csv)\n",
    "            train_start_time = df['datetime'].min() + pd.Timedelta(minutes=1)\n",
    "            train_end_time = df['datetime'].max() - pd.Timedelta(minutes=3)\n",
    "        elif '2.csv' in file:\n",
    "            # Remove the first 3 minutes and the last 2.5 minutes from the second training data (2.csv)\n",
    "            train_start_time = df['datetime'].min() + pd.Timedelta(minutes=3)\n",
    "            train_end_time = df['datetime'].max() - pd.Timedelta(minutes=2.5)\n",
    "        else:\n",
    "            # Remove the first and last 1.5 minutes from the other training data (4.csv)\n",
    "            train_start_time = df['datetime'].min() + pd.Timedelta(minutes=1.5)\n",
    "            train_end_time = df['datetime'].max() - pd.Timedelta(minutes=1.5)\n",
    "        processed_df = df[(df['datetime'] >= train_start_time) & (df['datetime'] <= train_end_time)]\n",
    "        processed_train_dfs.append(processed_df)\n",
    "    \n",
    "    # Concatenate all processed training dataframes\n",
    "    train_df = pd.concat(processed_train_dfs, ignore_index=True)\n",
    "    \n",
    "    # Split validation_test_df into validation and test sets\n",
    "    mid_index = len(validation_test_df) // 2\n",
    "    val_df = validation_test_df.iloc[:mid_index]\n",
    "    test_df = validation_test_df.iloc[mid_index:]\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Load datasets\n",
    "train_df, val_df, test_df = load_and_preprocess_data(train_files, validation_test_file)\n",
    "\n",
    "print('Loading complete')\n",
    "train_df.head(), val_df.head(), test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43538bb8-42d7-4205-945f-bb17dae9c7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences complete\n"
     ]
    }
   ],
   "source": [
    "# Define features and targets\n",
    "features = ['gyro_x', 'gyro_y', 'gyro_z', 'accel_x', 'accel_y', 'accel_z', 'x_cam', 'y_cam', 'z_cam']\n",
    "targets = ['x', 'y']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[features])\n",
    "val_X_scaled = scaler.transform(val_df[features])\n",
    "test_X_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# Extract target values\n",
    "train_y = train_df[targets].values\n",
    "val_y = val_df[targets].values\n",
    "test_y = test_df[targets].values\n",
    "\n",
    "# Convert the data into sequences\n",
    "def create_sequences(data, truth, sequence_length=800):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        labels.append(truth[i+sequence_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequence_length = 800\n",
    "X_train, y_train = create_sequences(train_X_scaled, train_y, sequence_length)\n",
    "X_val, y_val = create_sequences(val_X_scaled, val_y, sequence_length)\n",
    "X_test, y_test = create_sequences(test_X_scaled, test_y, sequence_length)\n",
    "\n",
    "print('Input sequences complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622a995-a7d4-438e-a26b-426d0872b276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 11:41:27.629447: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: /usr/lib/x86_64-linux-gnu/libcuda.so.1: file too short; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-14 11:41:27.629480: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 800, 50)           12000     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 800, 50)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 52        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,527\n",
      "Trainable params: 33,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 11:41:30.940619: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 9519379200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5165/5165 [==============================] - 3639s 703ms/step - loss: 239.0781 - val_loss: 227.6444\n",
      "Epoch 2/50\n",
      "2017/5165 [==========>...................] - ETA: 36:09 - loss: 178.1112"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(2))  # Output layer for the three targets (x, y, z)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Define input shape based on the training data\n",
    "input_shape = (sequence_length, len(features))\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape)\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Save the model\n",
    "model.save('trajectory_model15i.h5')\n",
    "\n",
    "print('Model training complete and saved to trajectory_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f39bf-4dbd-437e-ad8b-4a3e53d4925e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model and test it\n",
    "loaded_model = load_model('trajectory_model15i.h5')\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 0], label='True X')\n",
    "plt.plot(y_pred[:, 0], label='Predicted X')\n",
    "plt.legend()\n",
    "plt.title('X Trajectory')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 1], label='True Y')\n",
    "plt.plot(y_pred[:, 1], label='Predicted Y')\n",
    "plt.legend()\n",
    "plt.title('Y Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ca29d-dbf5-4ab8-bb5f-6e1bec61277f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# Load the model and test it\n",
    "loaded_model = load_model('trajectory_model15i.h5')\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_before_filter = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE before filtering: {rmse_before_filter}')\n",
    "\n",
    "# Apply a moving average filter to the predicted data\n",
    "def moving_average(data, window_size=100):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Apply moving average filter to each predicted dimension separately\n",
    "y_pred_filtered = np.array([moving_average(y_pred[:, i]) for i in range(y_pred.shape[1])]).T\n",
    "\n",
    "# Align the test data to match the length of the filtered predictions\n",
    "y_test_aligned = y_test[len(y_test) - len(y_pred_filtered):]\n",
    "\n",
    "# Calculate the RMSE again after filtering\n",
    "rmse_after_filter = np.sqrt(mean_squared_error(y_test_aligned, y_pred_filtered))\n",
    "print(f'RMSE after filtering: {rmse_after_filter}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 0], label='True X')\n",
    "#plt.plot(y_pred[:, 0], label='Predicted X')\n",
    "plt.plot(np.arange(len(y_test) - len(y_pred_filtered), len(y_test)), y_pred_filtered[:, 0], label='Filtered Predicted X')\n",
    "plt.legend()\n",
    "plt.title('X Trajectory')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 1], label='True Y')\n",
    "#plt.plot(y_pred[:, 1], label='Predicted Y')\n",
    "plt.plot(np.arange(len(y_test) - len(y_pred_filtered), len(y_test)), y_pred_filtered[:, 1], label='Filtered Predicted Y')\n",
    "plt.legend()\n",
    "plt.title('Y Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5196adb-7017-48ce-b962-737722a08eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
