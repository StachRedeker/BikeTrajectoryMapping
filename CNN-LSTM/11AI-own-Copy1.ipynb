{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5bce96-1628-4b59-8d36-de95a2e11305",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 11:17:17.672154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-26 11:17:18.378862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-26 11:17:18.378973: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-26 11:17:18.378979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                    date/time     gyro_x    gyro_y    gyro_z   accel_x  \\\n",
       " 0  2024-06-05 14:09:12.810708  47.229008  1.580153  4.488550  0.464475   \n",
       " 1  2024-06-05 14:09:12.818031  47.030534  0.419847  5.343511 -0.964863   \n",
       " 2  2024-06-05 14:09:12.824984  47.328244 -4.145038  6.557252 -0.799663   \n",
       " 3  2024-06-05 14:09:12.832316  48.068702 -3.725191  6.763359  1.841141   \n",
       " 4  2024-06-05 14:09:12.839155  47.679389 -1.847328  6.595420  3.287239   \n",
       " \n",
       "     accel_y    accel_z          x          y                   timestamp  \\\n",
       " 0  0.493206   7.393295  24.801591  53.530177  2024-06-05 14:09:12.810708   \n",
       " 1  0.105345   7.541735  24.776822  53.538791  2024-06-05 14:09:12.818031   \n",
       " 2  0.849942   8.384494  24.752052  53.547405  2024-06-05 14:09:12.824984   \n",
       " 3  0.289698  10.572795  24.727283  53.556018  2024-06-05 14:09:12.832316   \n",
       " 4  0.694318  11.298238  24.702514  53.564632  2024-06-05 14:09:12.839155   \n",
       " \n",
       "       x_cam     y_cam     z_cam                   datetime  \n",
       " 0  0.469269 -0.780899  0.338817 2024-06-05 14:09:12.810708  \n",
       " 1  0.469303 -0.780876  0.338859 2024-06-05 14:09:12.818031  \n",
       " 2  0.469336 -0.780852  0.338900 2024-06-05 14:09:12.824984  \n",
       " 3  0.469370 -0.780828  0.338941 2024-06-05 14:09:12.832316  \n",
       " 4  0.469404 -0.780804  0.338982 2024-06-05 14:09:12.839155  ,\n",
       "                     date/time      gyro_x     gyro_y     gyro_z   accel_x  \\\n",
       " 0  2024-06-12 10:39:01.386113    0.000000   0.297710   9.480916  0.000000   \n",
       " 1  2024-06-12 10:39:01.393501 -164.664122  47.977099  26.106870 -0.802058   \n",
       " 2  2024-06-12 10:39:01.400725 -197.076336  39.099237  24.137405 -0.897826   \n",
       " 3  2024-06-12 10:39:01.407921   47.549618  -0.908397  -0.427481 -0.718260   \n",
       " 4  2024-06-12 10:39:01.415058   48.938931  -1.068702  -1.389313 -0.866701   \n",
       " \n",
       "     accel_y   accel_z          x         y                   timestamp  \\\n",
       " 0  0.000000  0.000000  19.844030  7.616852  2024-06-12 10:39:01.386113   \n",
       " 1  0.426168  8.702923  19.844241  7.617198  2024-06-12 10:39:01.393501   \n",
       " 2  0.517148  8.765172  19.844453  7.617543  2024-06-12 10:39:01.400725   \n",
       " 3  0.402226  8.652645  19.844664  7.617888  2024-06-12 10:39:01.407921   \n",
       " 4  0.440533  8.762778  19.844875  7.618234  2024-06-12 10:39:01.415058   \n",
       " \n",
       "       x_cam     y_cam     z_cam                   datetime  \n",
       " 0 -0.983697 -0.179833 -0.000661 2024-06-12 10:39:01.386113  \n",
       " 1 -0.983713 -0.179845 -0.000703 2024-06-12 10:39:01.393501  \n",
       " 2 -0.983729 -0.179856 -0.000746 2024-06-12 10:39:01.400725  \n",
       " 3 -0.983745 -0.179868 -0.000788 2024-06-12 10:39:01.407921  \n",
       " 4 -0.983761 -0.179879 -0.000830 2024-06-12 10:39:01.415058  ,\n",
       "                         date/time     gyro_x    gyro_y    gyro_z   accel_x  \\\n",
       " 98932  2024-06-12 10:51:18.307869  38.709924  7.305344  9.229008 -1.731008   \n",
       " 98933  2024-06-12 10:51:18.315934  36.793893  5.114504  9.648855 -4.005499   \n",
       " 98934  2024-06-12 10:51:18.322856  35.381679  4.900763  6.129771 -3.873818   \n",
       " 98935  2024-06-12 10:51:18.329749  33.267176  4.603053  6.885496 -2.918532   \n",
       " 98936  2024-06-12 10:51:18.336650  36.557252  1.763359  6.259542 -3.043030   \n",
       " \n",
       "         accel_y   accel_z          x          y                   timestamp  \\\n",
       " 98932  0.826000  8.901642  15.442615  53.166534  2024-06-12 10:51:18.307869   \n",
       " 98933  1.920150  7.096414  15.420734  53.152238  2024-06-12 10:51:18.315934   \n",
       " 98934  0.703895  7.733271  15.398852  53.137941  2024-06-12 10:51:18.322856   \n",
       " 98935  0.028730  8.176199  15.376971  53.123645  2024-06-12 10:51:18.329749   \n",
       " 98936  0.191536  8.269573  15.355089  53.109348  2024-06-12 10:51:18.336650   \n",
       " \n",
       "              x_cam        y_cam       z_cam                   datetime  \n",
       " 98932 -1688.013761  1337.318013  311.222472 2024-06-12 10:51:18.307869  \n",
       " 98933 -1687.817928  1337.358629  311.222580 2024-06-12 10:51:18.315934  \n",
       " 98934 -1687.622096  1337.399244  311.222688 2024-06-12 10:51:18.322856  \n",
       " 98935 -1687.485264  1337.608474  311.223072 2024-06-12 10:51:18.329749  \n",
       " 98936 -1687.348433  1337.817704  311.223455 2024-06-12 10:51:18.336650  )"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the file paths\n",
    "train_files = ['own/data-int/1.csv', 'own/data-int/2.csv', 'own/data-int/5.csv']\n",
    "validation_test_file = 'own/data-int/4.csv'\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(train_files, validation_test_file):\n",
    "    # Load the training data\n",
    "    train_dfs = [pd.read_csv(file) for file in train_files]\n",
    "    \n",
    "    # Load the validation and test data\n",
    "    validation_test_df = pd.read_csv(validation_test_file)\n",
    "    \n",
    "    # Convert 'date/time' columns to datetime\n",
    "    for df in train_dfs:\n",
    "        df['datetime'] = pd.to_datetime(df['date/time'], errors='coerce')\n",
    "    validation_test_df['datetime'] = pd.to_datetime(validation_test_df['date/time'], errors='coerce')\n",
    "    \n",
    "    # Process each training dataframe\n",
    "    processed_train_dfs = []\n",
    "    for df, file in zip(train_dfs, train_files):\n",
    "        if '1.csv' in file:\n",
    "            # Remove the first 1 minute and the last 3 minutes from the first training data (1.csv)\n",
    "            train_start_time = df['datetime'].min() + pd.Timedelta(minutes=1)\n",
    "            train_end_time = df['datetime'].max() - pd.Timedelta(minutes=3)\n",
    "        elif '2.csv' in file:\n",
    "            # Remove the first 3 minutes and the last 2.5 minutes from the second training data (2.csv)\n",
    "            train_start_time = df['datetime'].min() + pd.Timedelta(minutes=3)\n",
    "            train_end_time = df['datetime'].max() - pd.Timedelta(minutes=2.5)\n",
    "        else:\n",
    "            # Remove the first and last 1.5 minutes from the other training data (4.csv)\n",
    "            train_start_time = df['datetime'].min() + pd.Timedelta(minutes=1.5)\n",
    "            train_end_time = df['datetime'].max() - pd.Timedelta(minutes=1.5)\n",
    "        processed_df = df[(df['datetime'] >= train_start_time) & (df['datetime'] <= train_end_time)]\n",
    "        processed_train_dfs.append(processed_df)\n",
    "    \n",
    "    # Concatenate all processed training dataframes\n",
    "    train_df = pd.concat(processed_train_dfs, ignore_index=True)\n",
    "    \n",
    "    # Split validation_test_df into validation and test sets\n",
    "    mid_index = len(validation_test_df) // 2\n",
    "    val_df = validation_test_df.iloc[:mid_index]\n",
    "    test_df = validation_test_df.iloc[mid_index:]\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Load datasets\n",
    "train_df, val_df, test_df = load_and_preprocess_data(train_files, validation_test_file)\n",
    "\n",
    "print('Loading complete')\n",
    "train_df.head(), val_df.head(), test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43538bb8-42d7-4205-945f-bb17dae9c7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences complete\n"
     ]
    }
   ],
   "source": [
    "# Define features and targets\n",
    "features = ['gyro_x', 'gyro_y', 'gyro_z', 'accel_x', 'accel_y', 'accel_z', 'x_cam', 'y_cam', 'z_cam']\n",
    "targets = ['x', 'y']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[features])\n",
    "val_X_scaled = scaler.transform(val_df[features])\n",
    "test_X_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# Extract target values\n",
    "train_y = train_df[targets].values\n",
    "val_y = val_df[targets].values\n",
    "test_y = test_df[targets].values\n",
    "\n",
    "# Convert the data into sequences\n",
    "def create_sequences(data, truth, sequence_length=3200):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        labels.append(truth[i+sequence_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequence_length = 3200\n",
    "X_train, y_train = create_sequences(train_X_scaled, train_y, sequence_length)\n",
    "X_val, y_val = create_sequences(val_X_scaled, val_y, sequence_length)\n",
    "X_test, y_test = create_sequences(test_X_scaled, test_y, sequence_length)\n",
    "\n",
    "print('Input sequences complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622a995-a7d4-438e-a26b-426d0872b276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 23:39:24.438540: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: /usr/lib/x86_64-linux-gnu/libcuda.so.1: file too short; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-16 23:39:24.438567: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 3198, 32)          896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 3198, 32)         128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 1599, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1599, 32)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1599, 48)          15552     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1599, 48)         192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 48)                18624     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                1176      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,618\n",
      "Trainable params: 36,458\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Checkpoint loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 23:39:38.547318: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 37801036800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2042/5128 [==========>...................] - ETA: 1:09:20 - loss: 65.0031 - rmse: 7.9028"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dropout, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model with fewer layers to reduce overfitting\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(sequence_length, len(features))),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    LSTM(48, return_sequences=True, kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    LSTM(48, kernel_regularizer='l2'),\n",
    "    Dropout(0.3),\n",
    "    Dense(24, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(len(targets))\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=[RootMeanSquaredError(name='rmse')])\n",
    "model.summary()\n",
    "\n",
    "# Define the checkpoint\n",
    "checkpoint_filepath = 'trajectory_model11i-checkpoint.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Load weights if checkpoint exists\n",
    "if os.path.exists(checkpoint_filepath):\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    print('Checkpoint loaded.')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, reduce_lr, checkpoint_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_rmse = model.evaluate(X_test, y_test)\n",
    "print(f'Test RMSE: {test_rmse:.4f} meters')\n",
    "\n",
    "# Export the model\n",
    "model.save('trajectory_model11i.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397f39bf-4dbd-437e-ad8b-4a3e53d4925e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at trajectory_model11i.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model and test it\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrajectory_model11i.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Predict using the test data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/legacy/save.py:227\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    233\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    234\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at trajectory_model11i.h5"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model and test it\n",
    "loaded_model = load_model('trajectory_model11i.h5')\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame to store the true and predicted values\n",
    "data = {\n",
    "    'true_x': y_test[:, 0],\n",
    "    'predicted_x': y_pred[:, 0],\n",
    "    'true_y': y_test[:, 1],\n",
    "    'predicted_y': y_pred[:, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('trajectory_predictions11i.csv', index=False)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 0], label='True X')\n",
    "plt.plot(y_pred[:, 0], label='Predicted X')\n",
    "plt.legend()\n",
    "plt.title('X Trajectory')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 1], label='True Y')\n",
    "plt.plot(y_pred[:, 1], label='Predicted Y')\n",
    "plt.legend()\n",
    "plt.title('Y Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ca29d-dbf5-4ab8-bb5f-6e1bec61277f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# Load the model and test it\n",
    "loaded_model = load_model('trajectory_model11i.h5')\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_before_filter = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE before filtering: {rmse_before_filter}')\n",
    "\n",
    "# Apply a moving average filter to the predicted data\n",
    "def moving_average(data, window_size=100):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Apply moving average filter to each predicted dimension separately\n",
    "y_pred_filtered = np.array([moving_average(y_pred[:, i]) for i in range(y_pred.shape[1])]).T\n",
    "\n",
    "# Align the test data to match the length of the filtered predictions\n",
    "y_test_aligned = y_test[len(y_test) - len(y_pred_filtered):]\n",
    "\n",
    "# Calculate the RMSE again after filtering\n",
    "rmse_after_filter = np.sqrt(mean_squared_error(y_test_aligned, y_pred_filtered))\n",
    "print(f'RMSE after filtering: {rmse_after_filter}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 0], label='True X')\n",
    "#plt.plot(y_pred[:, 0], label='Predicted X')\n",
    "plt.plot(np.arange(len(y_test) - len(y_pred_filtered), len(y_test)), y_pred_filtered[:, 0], label='Filtered Predicted X')\n",
    "plt.legend()\n",
    "plt.title('X Trajectory')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 1], label='True Y')\n",
    "#plt.plot(y_pred[:, 1], label='Predicted Y')\n",
    "plt.plot(np.arange(len(y_test) - len(y_pred_filtered), len(y_test)), y_pred_filtered[:, 1], label='Filtered Predicted Y')\n",
    "plt.legend()\n",
    "plt.title('Y Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5196adb-7017-48ce-b962-737722a08eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
