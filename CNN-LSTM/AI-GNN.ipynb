{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5bce96-1628-4b59-8d36-de95a2e11305",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 23:15:13.902516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 23:15:15.083308: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-14 23:15:15.083394: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-14 23:15:15.083402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the folder names\n",
    "TRAINING = ['MH_03_medium', 'V1_02_medium', 'V2_03_difficult', 'MH_02_easy', 'MH_04_difficult', 'V1_01_easy', 'V1_03_difficult', 'V2_02_medium']\n",
    "VALIDATION = ['MH_05_difficult', 'V2_01_easy']\n",
    "TEST = ['MH_01_easy']\n",
    "\n",
    "# Function to load data and append camera results\n",
    "def load_data(folders):\n",
    "    data_dict = {}\n",
    "    truth_dict = {}\n",
    "    for name in folders:\n",
    "        data_path = f'EuRoC/{name}/mav0/imu0/data.csv'\n",
    "        truth_path = f'EuRoC/{name}/mav0/state_groundtruth_estimate0/data.csv'\n",
    "        camera_path = f'camera/{name}-results.csv'\n",
    "        \n",
    "        # Load the datasets\n",
    "        data_df = pd.read_csv(data_path)\n",
    "        truth_df = pd.read_csv(truth_path)\n",
    "        camera_df = pd.read_csv(camera_path)\n",
    "        \n",
    "        # Interpolate camera data to match the length of data_df\n",
    "        new_index = np.linspace(0, len(camera_df) - 1, len(data_df))\n",
    "        camera_interpolated = camera_df.reindex(new_index).interpolate(method='linear')\n",
    "        \n",
    "        # Reset index to align properly\n",
    "        camera_interpolated.reset_index(drop=True, inplace=True)\n",
    "        data_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Combine the dataframes\n",
    "        combined_df = pd.concat([data_df, camera_interpolated], axis=1)\n",
    "        \n",
    "        # Store in dictionaries\n",
    "        data_dict[name] = combined_df\n",
    "        truth_dict[name] = truth_df\n",
    "    \n",
    "    return data_dict, truth_dict\n",
    "\n",
    "# Load datasets\n",
    "train_data_dict, train_truth_dict = load_data(TRAINING)\n",
    "val_data_dict, val_truth_dict = load_data(VALIDATION)\n",
    "test_data_dict, test_truth_dict = load_data(TEST)\n",
    "\n",
    "# Combine datasets\n",
    "train_data = pd.concat(train_data_dict.values())\n",
    "train_truth = pd.concat(train_truth_dict.values())\n",
    "val_data = pd.concat(val_data_dict.values())\n",
    "val_truth = pd.concat(val_truth_dict.values())\n",
    "test_data = pd.concat(test_data_dict.values())\n",
    "test_truth = pd.concat(test_truth_dict.values())\n",
    "\n",
    "# Clean up column names to remove leading and trailing spaces\n",
    "train_truth.columns = train_truth.columns.str.strip()\n",
    "val_truth.columns = val_truth.columns.str.strip()\n",
    "test_truth.columns = test_truth.columns.str.strip()\n",
    "\n",
    "print('Loading complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43538bb8-42d7-4205-945f-bb17dae9c7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequences complete\n"
     ]
    }
   ],
   "source": [
    "# Define features and targets\n",
    "features = ['w_RS_S_x [rad s^-1]', 'w_RS_S_y [rad s^-1]', 'w_RS_S_z [rad s^-1]', \n",
    "            'a_RS_S_x [m s^-2]', 'a_RS_S_y [m s^-2]', 'a_RS_S_z [m s^-2]', 'x', 'y', 'z']\n",
    "targets = ['p_RS_R_x [m]', 'p_RS_R_y [m]', 'p_RS_R_z [m]']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_data[features])\n",
    "val_X_scaled = scaler.transform(val_data[features])\n",
    "test_X_scaled = scaler.transform(test_data[features])\n",
    "\n",
    "# Align lengths by trimming the excess IMU data entries\n",
    "aligned_length = min(len(train_X_scaled), len(train_truth[targets].values))\n",
    "train_X_scaled = train_X_scaled[:aligned_length]\n",
    "train_truth_aligned = train_truth[targets].values[:aligned_length]\n",
    "\n",
    "aligned_length_val = min(len(val_X_scaled), len(val_truth[targets].values))\n",
    "val_X_scaled = val_X_scaled[:aligned_length_val]\n",
    "val_truth_aligned = val_truth[targets].values[:aligned_length_val]\n",
    "\n",
    "aligned_length_test = min(len(test_X_scaled), len(test_truth[targets].values))\n",
    "test_X_scaled = test_X_scaled[:aligned_length_test]\n",
    "test_truth_aligned = test_truth[targets].values[:aligned_length_test]\n",
    "\n",
    "# Convert the data into sequences\n",
    "def create_sequences(data, truth, sequence_length=50):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        labels.append(truth[i+sequence_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequence_length = 50\n",
    "X_train, y_train = create_sequences(train_X_scaled, train_truth_aligned, sequence_length)\n",
    "X_val, y_val = create_sequences(val_X_scaled, val_truth_aligned, sequence_length)\n",
    "X_test, y_test = create_sequences(test_X_scaled, test_truth_aligned, sequence_length)\n",
    "\n",
    "print('input sequences complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622a995-a7d4-438e-a26b-426d0872b276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 23:15:24.597120: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: /usr/lib/x86_64-linux-gnu/libcuda.so.1: file too short; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-14 23:15:24.597144: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 50, 100)           44000     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 100)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 129,603\n",
      "Trainable params: 129,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 1.2029\n",
      "Epoch 1: val_loss improved from inf to 16.49616, saving model to trajectory_prediction_model-checkpoint.h5\n",
      "2944/2944 [==============================] - 210s 70ms/step - loss: 1.2029 - val_loss: 16.4962\n",
      "Epoch 2/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.5100\n",
      "Epoch 2: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 210s 71ms/step - loss: 0.5100 - val_loss: 19.7919\n",
      "Epoch 3/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.2840\n",
      "Epoch 3: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 211s 72ms/step - loss: 0.2840 - val_loss: 18.1674\n",
      "Epoch 4/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.2114\n",
      "Epoch 4: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 210s 71ms/step - loss: 0.2114 - val_loss: 19.1797\n",
      "Epoch 5/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.1678\n",
      "Epoch 5: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 209s 71ms/step - loss: 0.1678 - val_loss: 19.9015\n",
      "Epoch 6/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.1651\n",
      "Epoch 6: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 212s 72ms/step - loss: 0.1651 - val_loss: 19.1730\n",
      "Epoch 7/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.1217\n",
      "Epoch 7: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 214s 73ms/step - loss: 0.1217 - val_loss: 19.2132\n",
      "Epoch 8/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 8: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 212s 72ms/step - loss: 0.1085 - val_loss: 18.3912\n",
      "Epoch 9/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.1273\n",
      "Epoch 9: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 214s 73ms/step - loss: 0.1273 - val_loss: 19.0799\n",
      "Epoch 10/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 10: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 212s 72ms/step - loss: 0.1004 - val_loss: 18.9376\n",
      "Epoch 11/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0808\n",
      "Epoch 11: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 209s 71ms/step - loss: 0.0808 - val_loss: 19.2039\n",
      "Epoch 12/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0799\n",
      "Epoch 12: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 214s 73ms/step - loss: 0.0799 - val_loss: 19.5246\n",
      "Epoch 13/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0713\n",
      "Epoch 13: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 215s 73ms/step - loss: 0.0713 - val_loss: 19.1807\n",
      "Epoch 14/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0742\n",
      "Epoch 14: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 215s 73ms/step - loss: 0.0742 - val_loss: 20.5493\n",
      "Epoch 15/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0554\n",
      "Epoch 15: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 214s 73ms/step - loss: 0.0554 - val_loss: 20.2637\n",
      "Epoch 16/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0632\n",
      "Epoch 16: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 216s 73ms/step - loss: 0.0632 - val_loss: 20.6500\n",
      "Epoch 17/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0674\n",
      "Epoch 17: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 216s 73ms/step - loss: 0.0674 - val_loss: 19.8706\n",
      "Epoch 18/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0534\n",
      "Epoch 18: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 214s 73ms/step - loss: 0.0534 - val_loss: 19.5809\n",
      "Epoch 19/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0572\n",
      "Epoch 19: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 216s 73ms/step - loss: 0.0572 - val_loss: 19.6534\n",
      "Epoch 20/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0613\n",
      "Epoch 20: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 216s 73ms/step - loss: 0.0613 - val_loss: 21.1160\n",
      "Epoch 21/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0493\n",
      "Epoch 21: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 216s 74ms/step - loss: 0.0493 - val_loss: 19.7942\n",
      "Epoch 22/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0449\n",
      "Epoch 22: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 214s 73ms/step - loss: 0.0449 - val_loss: 19.8073\n",
      "Epoch 23/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 23: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 212s 72ms/step - loss: 0.0420 - val_loss: 19.7320\n",
      "Epoch 24/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0619\n",
      "Epoch 24: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 214s 73ms/step - loss: 0.0619 - val_loss: 20.2146\n",
      "Epoch 25/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0358\n",
      "Epoch 25: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 215s 73ms/step - loss: 0.0358 - val_loss: 20.7083\n",
      "Epoch 26/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0399\n",
      "Epoch 26: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 218s 74ms/step - loss: 0.0399 - val_loss: 19.7437\n",
      "Epoch 27/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0382\n",
      "Epoch 27: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 217s 74ms/step - loss: 0.0382 - val_loss: 20.6168\n",
      "Epoch 28/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0424\n",
      "Epoch 28: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 217s 74ms/step - loss: 0.0424 - val_loss: 19.7389\n",
      "Epoch 29/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0351\n",
      "Epoch 29: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 213s 72ms/step - loss: 0.0351 - val_loss: 18.8523\n",
      "Epoch 30/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0345\n",
      "Epoch 30: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 212s 72ms/step - loss: 0.0345 - val_loss: 19.5872\n",
      "Epoch 31/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0354\n",
      "Epoch 31: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 215s 73ms/step - loss: 0.0354 - val_loss: 20.2798\n",
      "Epoch 32/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0291\n",
      "Epoch 32: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 217s 74ms/step - loss: 0.0291 - val_loss: 19.6506\n",
      "Epoch 33/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 33: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 219s 74ms/step - loss: 0.0329 - val_loss: 19.7143\n",
      "Epoch 34/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0337\n",
      "Epoch 34: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 217s 74ms/step - loss: 0.0337 - val_loss: 20.2384\n",
      "Epoch 35/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0627\n",
      "Epoch 35: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 214s 73ms/step - loss: 0.0627 - val_loss: 19.2285\n",
      "Epoch 36/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0350\n",
      "Epoch 36: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 215s 73ms/step - loss: 0.0350 - val_loss: 19.4625\n",
      "Epoch 37/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0361\n",
      "Epoch 37: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 217s 74ms/step - loss: 0.0361 - val_loss: 19.1841\n",
      "Epoch 38/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0278\n",
      "Epoch 38: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 219s 75ms/step - loss: 0.0278 - val_loss: 19.5301\n",
      "Epoch 39/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 39: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 218s 74ms/step - loss: 0.0267 - val_loss: 19.7888\n",
      "Epoch 40/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0325\n",
      "Epoch 40: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 219s 74ms/step - loss: 0.0325 - val_loss: 21.1847\n",
      "Epoch 41/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0298\n",
      "Epoch 41: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 220s 75ms/step - loss: 0.0298 - val_loss: 20.2931\n",
      "Epoch 42/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0300\n",
      "Epoch 42: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 218s 74ms/step - loss: 0.0300 - val_loss: 20.7033\n",
      "Epoch 43/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 43: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 218s 74ms/step - loss: 0.0238 - val_loss: 20.5141\n",
      "Epoch 44/50\n",
      "2944/2944 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 44: val_loss did not improve from 16.49616\n",
      "2944/2944 [==============================] - 217s 74ms/step - loss: 0.0241 - val_loss: 20.0575\n",
      "Epoch 45/50\n",
      "2398/2944 [=======================>......] - ETA: 36s - loss: 0.0257"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(3))  # Output layer for the three targets (x, y, z)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Define input shape based on the training data\n",
    "input_shape = (sequence_length, len(features))\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape)\n",
    "model.summary()\n",
    "\n",
    "# Define the checkpoint\n",
    "checkpoint_filepath = 'trajectory_prediction_model-checkpoint.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Load weights if checkpoint exists\n",
    "if os.path.exists(checkpoint_filepath):\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    print('Checkpoint loaded.')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint_callback],\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Save the model\n",
    "model.save('trajectory_prediction_model.h5')\n",
    "\n",
    "print('Model training complete and saved to trajectory_prediction_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f39bf-4dbd-437e-ad8b-4a3e53d4925e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model and test it\n",
    "loaded_model = load_model('trajectory_prediction_model.h5')\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 0], label='True X')\n",
    "plt.plot(y_pred[:, 0], label='Predicted X')\n",
    "plt.legend()\n",
    "plt.title('X Trajectory')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 1], label='True Y')\n",
    "plt.plot(y_pred[:, 1], label='Predicted Y')\n",
    "plt.legend()\n",
    "plt.title('Y Trajectory')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 2], label='True Z')\n",
    "plt.plot(y_pred[:, 2], label='Predicted Z')\n",
    "plt.legend()\n",
    "plt.title('Z Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ca29d-dbf5-4ab8-bb5f-6e1bec61277f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model and test it\n",
    "loaded_model = load_model('trajectory_prediction_model.h5')\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_before_filter = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE before filtering: {rmse_before_filter}')\n",
    "\n",
    "# Apply a moving average filter to the predicted data\n",
    "def moving_average(data, window_size=250):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Apply moving average filter to each predicted dimension separately\n",
    "y_pred_filtered = np.array([moving_average(y_pred[:, i]) for i in range(y_pred.shape[1])]).T\n",
    "\n",
    "# Align the test data to match the length of the filtered predictions\n",
    "y_test_aligned = y_test[len(y_test) - len(y_pred_filtered):]\n",
    "\n",
    "# Calculate the RMSE again after filtering\n",
    "rmse_after_filter = np.sqrt(mean_squared_error(y_test_aligned, y_pred_filtered))\n",
    "print(f'RMSE after filtering: {rmse_after_filter}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 0], label='True X')\n",
    "plt.plot(y_pred[:, 0], label='Predicted X')\n",
    "plt.plot(y_pred_filtered[:, 0], label='Filtered Predicted X')\n",
    "plt.legend()\n",
    "plt.title('X Trajectory')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 1], label='True Y')\n",
    "plt.plot(y_pred[:, 1], label='Predicted Y')\n",
    "plt.plot(y_pred_filtered[:, 1], label='Filtered Predicted Y')\n",
    "plt.legend()\n",
    "plt.title('Y Trajectory')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test[:, 2], label='True Z')\n",
    "plt.plot(y_pred[:, 2], label='Predicted Z')\n",
    "plt.plot(y_pred_filtered[:, 2], label='Filtered Predicted Z')\n",
    "plt.legend()\n",
    "plt.title('Z Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05f893-9fd7-4d10-9d37-2f58bef57e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# 3D Plot of the trajectories\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot True Trajectory\n",
    "ax.plot(y_test[:, 0], y_test[:, 1], y_test[:, 2], label='True Trajectory', color='blue')\n",
    "\n",
    "# Plot Predicted Trajectory\n",
    "#ax.plot(y_pred[:, 0], y_pred[:, 1], y_pred[:, 2], label='Predicted Trajectory', color='red')\n",
    "\n",
    "# Plot Filtered Predicted Trajectory\n",
    "ax.plot(y_pred_filtered[:, 0], y_pred_filtered[:, 1], y_pred_filtered[:, 2], label='Filtered Predicted Trajectory', color='green')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.legend()\n",
    "plt.title('3D Trajectory Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5196adb-7017-48ce-b962-737722a08eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
