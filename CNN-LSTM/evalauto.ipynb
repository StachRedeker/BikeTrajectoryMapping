{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f82b67-5efb-4ab6-bf03-704da43fa754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:55:45.891312: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 14:55:47.071988: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-17 14:55:47.072077: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-06-17 14:55:47.072084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing all data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data files: 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]\n",
      "Loading and preprocessing interpolated data files: 100%|██████████| 3/3 [00:01<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import correlate\n",
    "import concurrent.futures\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define sequence length table\n",
    "sequence_length_table = {\n",
    "    '2': 200, '3': 400, '4': 400, '5': 200, '5i': 200, '6': 200, '6i': 200,\n",
    "    '7': 100, '7i': 100, '8': 800, '8i': 800, '9': 800, '9i': 800, '10': 1600, '10i': 1600,\n",
    "    '11': 3200, '11i': 3200, '12': 400, '12i': 400, '13': 200, '13i': 200, '14': 100, '14i': 100,\n",
    "    '15': 800, '15i': 800\n",
    "}\n",
    "\n",
    "# Define file paths\n",
    "data_files = {\n",
    "    \"Small trajectory test\": 'own/data/4.csv',\n",
    "    \"Real traffic test\": 'own/data/6.csv',\n",
    "    \"Zero-shot large trajectory test\": 'own/data/7.csv'\n",
    "}\n",
    "\n",
    "data_int_files = {\n",
    "    \"Small trajectory test\": 'own/data-int/4.csv',\n",
    "    \"Real traffic test\": 'own/data-int/6.csv',\n",
    "    \"Zero-shot large trajectory test\": 'own/data-int/7.csv'\n",
    "}\n",
    "\n",
    "# Function to load and preprocess a single file\n",
    "def load_and_preprocess_file(file, start_offset, end_offset):\n",
    "    df = pd.read_csv(file)\n",
    "    df['datetime'] = pd.to_datetime(df['date/time'], errors='coerce')\n",
    "    start_time = df['datetime'].min() + pd.Timedelta(minutes=start_offset)\n",
    "    end_time = df['datetime'].max() - pd.Timedelta(minutes=end_offset)\n",
    "    return df[(df['datetime'] >= start_time) & (df['datetime'] <= end_time)]\n",
    "\n",
    "# Function to load and preprocess all files\n",
    "def load_and_preprocess_all_files():\n",
    "    all_data = {}\n",
    "    for key, file in tqdm(data_files.items(), desc=\"Loading and preprocessing data files\"):\n",
    "        if \"4.csv\" in file:\n",
    "            start_offset = 1.5\n",
    "            end_offset = 1.5\n",
    "        elif \"6.csv\" in file:\n",
    "            start_offset = 2\n",
    "            end_offset = 1\n",
    "        else:\n",
    "            start_offset = 1\n",
    "            end_offset = 1\n",
    "\n",
    "        df = load_and_preprocess_file(file, start_offset, end_offset)\n",
    "        all_data[key] = df\n",
    "\n",
    "    for key, file in tqdm(data_int_files.items(), desc=\"Loading and preprocessing interpolated data files\"):\n",
    "        if \"4.csv\" in file:\n",
    "            start_offset = 1.5\n",
    "            end_offset = 1.5\n",
    "        elif \"6.csv\" in file:\n",
    "            start_offset = 2\n",
    "            end_offset = 1\n",
    "        else:\n",
    "            start_offset = 1\n",
    "            end_offset = 1\n",
    "\n",
    "        df = load_and_preprocess_file(file, start_offset, end_offset)\n",
    "        all_data[key + \" (interpolated)\"] = df\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Load and preprocess all data\n",
    "print(\"Loading and preprocessing all data...\")\n",
    "all_data = load_and_preprocess_all_files()\n",
    "print(\"Data loading and preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a1f24d-7552-42f4-b036-1d32186fcb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing features: 100%|██████████| 6/6 [00:00<00:00, 65.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature standardization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardize features\n",
    "features = ['gyro_x', 'gyro_y', 'gyro_z', 'accel_x', 'accel_y', 'accel_z', 'x_cam', 'y_cam', 'z_cam']\n",
    "targets = ['x', 'y']\n",
    "\n",
    "scalers = {}\n",
    "for key in tqdm(all_data, desc=\"Standardizing features\"):\n",
    "    scaler = StandardScaler()\n",
    "    all_data[key][features] = scaler.fit_transform(all_data[key][features])\n",
    "    scalers[key] = scaler\n",
    "\n",
    "print(\"Feature standardization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db2a46-6a31-4142-8b8d-a74cd9201be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for model files...\n",
      "Found 20 model files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences by sequence length:   0%|          | 0/6 [00:00<?, ?it/s]\n",
      "Sequence length 100:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length 100:   8%|▊         | 1/12 [28:41<5:15:35, 1721.41s/it]\u001b[A\n",
      "Sequence length 100:  17%|█▋        | 2/12 [28:50<1:59:03, 714.36s/it] \u001b[A\n",
      "Sequence length 100:  25%|██▌       | 3/12 [28:52<58:21, 389.04s/it]  \u001b[A\n",
      "Sequence length 100:  33%|███▎      | 4/12 [29:06<32:06, 240.87s/it]\u001b[A\n",
      "Sequence length 100:  42%|████▏     | 5/12 [45:30<59:21, 508.84s/it]\u001b[A\n",
      "Sequence length 100:  50%|█████     | 6/12 [45:43<34:00, 340.12s/it]\u001b[A\n",
      "Sequence length 100:  58%|█████▊    | 7/12 [45:56<19:26, 233.29s/it]\u001b[A\n",
      "Sequence length 100:  67%|██████▋   | 8/12 [46:01<10:42, 160.73s/it]\u001b[A\n",
      "Sequence length 100:  75%|███████▌  | 9/12 [47:23<06:47, 135.84s/it]\u001b[A\n",
      "Sequence length 100:  83%|████████▎ | 10/12 [47:23<03:08, 94.13s/it]\u001b[A\n",
      "Sequence length 100:  92%|█████████▏| 11/12 [47:25<01:05, 65.76s/it]\u001b[A\n",
      "Sequence length 100: 100%|██████████| 12/12 [47:25<00:00, 237.16s/it][A\n",
      "Creating sequences by sequence length:  17%|█▋        | 1/6 [47:26<3:57:13, 2846.77s/it]\n",
      "Sequence length 200:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Sequence length 200:   6%|▌         | 1/18 [1:17:04<21:50:18, 4624.64s/it]\u001b[A\n",
      "Sequence length 200:  11%|█         | 2/18 [1:17:09<8:28:30, 1906.93s/it] \u001b[A\n",
      "Sequence length 200:  17%|█▋        | 3/18 [1:17:30<4:21:30, 1046.02s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Function to get sequence length from the model name\n",
    "def get_sequence_length_from_model_name(model_name):\n",
    "    base_name = os.path.basename(model_name).replace('trajectory_model', '').replace('.h5', '')\n",
    "    return sequence_length_table.get(base_name, None)\n",
    "\n",
    "# Function to create sequences for each dataset\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length][features].values)\n",
    "        labels.append(data.iloc[i+sequence_length][targets].values)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Function to create sequences with multithreading\n",
    "def create_sequences_for_key(key, seq_len):\n",
    "    sequences, labels = create_sequences(all_data[key], seq_len)\n",
    "    return key, sequences, labels\n",
    "\n",
    "# Store sequences and labels in a dictionary\n",
    "sequences_labels = {}\n",
    "\n",
    "# Find all .h5 model files in the root folder\n",
    "def find_model_files(root_folder):\n",
    "    model_files = []\n",
    "    for file in os.listdir(root_folder):\n",
    "        if file.endswith(\".h5\") and \"checkpoint\" not in file:\n",
    "            model_files.append(os.path.join(root_folder, file))\n",
    "    return model_files\n",
    "\n",
    "# Find all model files\n",
    "print(\"Searching for model files...\")\n",
    "root_folder = \".\"\n",
    "model_files = find_model_files(root_folder)\n",
    "print(f\"Found {len(model_files)} model files.\")\n",
    "\n",
    "# Filter out models not in sequence_length_table\n",
    "model_files = [mf for mf in model_files if get_sequence_length_from_model_name(mf) is not None]\n",
    "\n",
    "# Group datasets by sequence length\n",
    "datasets_by_seq_len = {}\n",
    "for model_file in model_files:\n",
    "    model_name = os.path.basename(model_file)\n",
    "    seq_len = get_sequence_length_from_model_name(model_name)\n",
    "    if seq_len not in datasets_by_seq_len:\n",
    "        datasets_by_seq_len[seq_len] = []\n",
    "    \n",
    "    if 'i' in model_name:\n",
    "        datasets_by_seq_len[seq_len].extend([key + \" (interpolated)\" for key in data_files.keys()])\n",
    "    else:\n",
    "        datasets_by_seq_len[seq_len].extend(data_files.keys())\n",
    "\n",
    "# Create sequences using multithreading\n",
    "for seq_len, keys in tqdm(datasets_by_seq_len.items(), desc=\"Creating sequences by sequence length\"):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(create_sequences_for_key, key, seq_len): key for key in keys}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=f\"Sequence length {seq_len}\"):\n",
    "            key, sequences, labels = future.result()\n",
    "            sequences_labels[key] = (sequences, labels)\n",
    "\n",
    "print(\"Sequence creation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b425b-9be1-41ca-8c84-584046aa7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find all .h5 model files\n",
    "def find_model_files(root_folder):\n",
    "    model_files = []\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".h5\") and \"checkpoint\" not in file:\n",
    "                model_files.append(os.path.join(root, file))\n",
    "    return model_files\n",
    "\n",
    "# Find all model files\n",
    "print(\"Searching for model files...\")\n",
    "root_folder = \".\"\n",
    "model_files = find_model_files(root_folder)\n",
    "print(f\"Found {len(model_files)} model files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bec9a5-5e8f-4269-9dc9-bed07c62604b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate RMSE\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Function to apply a moving average filter to the predicted data\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Function to process data after filtering, scaling, shifting, and lag correction\n",
    "def process_data(y_true, y_pred, window_size):\n",
    "    print(f\"Processing data with window size {window_size}...\")\n",
    "    \n",
    "    # Apply moving average filter to predicted data\n",
    "    y_pred_filtered = np.array([moving_average(y_pred[:, i], window_size) for i in range(y_pred.shape[1])]).T\n",
    "\n",
    "    # Align the test data to match the length of the filtered predictions\n",
    "    y_test_aligned = y_true[len(y_true) - len(y_pred_filtered):]\n",
    "\n",
    "    # Calculate scaling factors using standard deviation\n",
    "    scaling_factors = np.std(y_test_aligned, axis=0) / np.std(y_pred_filtered, axis=0)\n",
    "\n",
    "    # Apply scaling factors to the filtered predicted data\n",
    "    y_pred_scaled = y_pred_filtered * scaling_factors\n",
    "\n",
    "    # Calculate the mean of the aligned ground truth and scaled predictions\n",
    "    mean_y_test = np.mean(y_test_aligned, axis=0)\n",
    "    mean_y_pred = np.mean(y_pred_scaled, axis=0)\n",
    "\n",
    "    # Calculate the shift required to align the means\n",
    "    shift = mean_y_test - mean_y_pred\n",
    "\n",
    "    # Apply the shift to the scaled predicted data\n",
    "    y_pred_scaled_shifted = y_pred_scaled + shift\n",
    "\n",
    "    # Determine the lag that maximizes the cross-correlation between the ground truth and the predicted data\n",
    "    lag_x = find_best_shift(y_test_aligned[:, 0], y_pred_scaled_shifted[:, 0])\n",
    "    lag_y = find_best_shift(y_test_aligned[:, 1], y_pred_scaled_shifted[:, 1])\n",
    "\n",
    "    # Create copies of the data to apply the shifts\n",
    "    y_pred_final_x = np.roll(y_pred_scaled_shifted[:, 0], shift=lag_x)\n",
    "    y_pred_final_y = np.roll(y_pred_scaled_shifted[:, 1], shift=lag_y)\n",
    "\n",
    "    # Combine the shifted dimensions back into a single array\n",
    "    y_pred_final = np.column_stack((y_pred_final_x, y_pred_final_y))\n",
    "\n",
    "    return y_test_aligned, y_pred_final, scaling_factors, shift, lag_x, lag_y\n",
    "\n",
    "# Function to evaluate a model on a dataset\n",
    "def evaluate_model(model_path, dataset_key, window_sizes):\n",
    "    print(f\"Evaluating model {model_path} on {dataset_key}...\")\n",
    "    model = load_model(model_path)\n",
    "    X, y = sequences_labels[dataset_key]\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        print(f\"Evaluating with window size {window_size}...\")\n",
    "        \n",
    "        # RMSE without any filtering\n",
    "        rmse_no_filter = calculate_rmse(y, y_pred)\n",
    "        \n",
    "        # RMSE after filtering\n",
    "        y_true_filtered, y_pred_filtered, _, _, _, _ = process_data(y, y_pred, window_size)\n",
    "        rmse_filter = calculate_rmse(y_true_filtered, y_pred_filtered)\n",
    "        \n",
    "        # RMSE after scaling, filtering, and shifting\n",
    "        y_true_final, y_pred_final, scaling_factors, shift, lag_x, lag_y = process_data(y, y_pred, window_size)\n",
    "        rmse_final = calculate_rmse(y_true_final, y_pred_final)\n",
    "        \n",
    "        # Error between expected end point and predicted end point\n",
    "        expected_end_point = y_true_final[-1]\n",
    "        predicted_end_point = y_pred_final[-1]\n",
    "        end_point_error = np.sqrt((expected_end_point[0] - predicted_end_point[0])**2 + (expected_end_point[1] - predicted_end_point[1])**2)\n",
    "\n",
    "        # Create plots\n",
    "        plot_trajectories(y, y_pred, f\"{dataset_key} ({os.path.basename(model_path)}, window size={window_size})\", save_path=f\"images/{dataset_key}_{os.path.basename(model_path)}_ws{window_size}.png\")\n",
    "        \n",
    "        result = {\n",
    "            \"model\": os.path.basename(model_path),\n",
    "            \"test\": dataset_key,\n",
    "            \"window_size\": window_size,\n",
    "            \"rmse_no_filter\": rmse_no_filter,\n",
    "            \"rmse_filter\": rmse_filter,\n",
    "            \"rmse_final\": rmse_final,\n",
    "            \"end_point_error\": end_point_error\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to find the best shift\n",
    "def find_best_shift(y_true, y_pred):\n",
    "    correlation = correlate(y_true, y_pred)\n",
    "    lag = correlation.argmax() - (len(y_pred) - 1)\n",
    "    return lag\n",
    "\n",
    "# Function to plot trajectories\n",
    "def plot_trajectories(true_data, pred_data, title_suffix, save_path=None):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(true_data[:, 0], label=f'True X ({title_suffix})')\n",
    "    plt.plot(pred_data[:, 0], label=f'Predicted X ({title_suffix})')\n",
    "    plt.legend()\n",
    "    plt.title(f'X Trajectory ({title_suffix})')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path.replace('.png', '_x.png'))\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(true_data[:, 1], label=f'True Y ({title_suffix})')\n",
    "    plt.plot(pred_data[:, 1], label=f'Predicted Y ({title_suffix})')\n",
    "    plt.legend()\n",
    "    plt.title(f'Y Trajectory ({title_suffix})')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path.replace('.png', '_y.png'))\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(true_data[:, 0], true_data[:, 1], label=f'True Trajectory ({title_suffix})', color='blue')\n",
    "    plt.plot(pred_data[:, 0], pred_data[:, 1], label=f'Processed Predicted Trajectory ({title_suffix})', color='red', linestyle='dashed')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.title(f'Trajectory Comparison ({title_suffix})')\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path.replace('.png', '_xy.png'))\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Define window sizes\n",
    "window_sizes = [200, 400, 800, 1600, 2500, 3200]\n",
    "\n",
    "# Function to run evaluations\n",
    "def run_evaluations():\n",
    "    results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for model_file in model_files:\n",
    "            model_name = os.path.basename(model_file)\n",
    "            if 'i' in model_name:\n",
    "                for test in data_int_files.keys():\n",
    "                    futures.append(executor.submit(evaluate_model, model_file, test + \" (interpolated)\", window_sizes))\n",
    "            else:\n",
    "                for test in data_files.keys():\n",
    "                    futures.append(executor.submit(evaluate_model, model_file, test, window_sizes))\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Evaluating models\"):\n",
    "            try:\n",
    "                results.extend(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluations\n",
    "print(\"Running evaluations...\")\n",
    "results = run_evaluations()\n",
    "print(\"Evaluations complete.\")\n",
    "\n",
    "# Write results to CSV\n",
    "print(\"Writing results to evaluation.csv...\")\n",
    "with open('evaluation.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['model', 'test', 'window_size', 'rmse_no_filter', 'rmse_filter', 'rmse_final', 'end_point_error']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for result in tqdm(results, desc=\"Writing results to CSV\"):\n",
    "        writer.writerow(result)\n",
    "print(\"Results written to evaluation.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
